\documentclass[10pt,conference,compsocconf]{IEEEtran}

%\usepackage{times}
%\usepackage{balance}
\usepackage{url}
\usepackage{graphicx}	% For figure environment


\begin{document}
\title{Image Compression with Neural Networks and SVD}

\author{
  David Chettrit\\
  Stephan Seebacher\\
  Sezer G\"uler\\
  Department of Computer Science, ETH Zurich, Switzerland
}

\bibliographystyle{plain}
\bibliography{howto-paper.bib}

\maketitle

\begin{abstract}

\end{abstract}

\section{Introduction}



\section{The Structure of a Paper}
\label{sec:structure-paper}



\section{Preliminaries}
\label{sec:tips-writing}

\subsection{Neural Networks}
Inspired by biological nervous systems, such as the human brain, artificial neural networks are used for information processing. In biological nervous systems, highly interconnected neurons work together to solve a specific task. Each neuron solves a subtask and communicates with other neurons. In artificial neural networks this idea is used to approximate arbitrary  non-linear functions. The nodes(neurons) are connected by weighted edges and the topology is chosen depending on the problem. In the training phase, the weights of the edges are set depending on the data. Neural networks are often represented in layers. Each layer contains some of the nodes and then follows the interconnections and then the next layer. 
\\
wieso verwendet man denn gerade Neuronale netzwerke....
\subsection{Neural Network Structure}
\begin{figure}[tbp]
  \centering
  \includegraphics[width=\columnwidth]{nnStructure}
  \caption{Neural Network Structure for compression.}
  \label{fig:nnStructure}
\end{figure}

Figure~\ref{fig:nnStructure} shows the neural network structure we use for compression. There are three layers: input layer, hidden layer and output layer. The input layer and the hidden layer are fully interconnected as are the hidden layer and the output layer. The weights of the edges are detrermined by the training algorithm which is described in the next section. 

\subsection{Neural Network Training Algorithm}
For the training 8 by 8 chunks of the image are chosen uniformly at random. The output of the bottleneck-like neural network has to be the same as the input and the weights are set accordingly. This process is iterated until the image is reconstructed faithfully.  
Different learning methods were tried like (....TODO) but it turned out that for the Levenberg-Marquardt method the results were best. 
\subsection{Singular Value Decomposition}
The singular value decomposition(SVD) is a matrix factorization. A m$\times$n matrix M can be written as a product of three matrices U, $\Sigma$ and V$^*$, where U is a unitary m$\times$m matrix, V$^*$ is the conjugate transpose of a n$\times$n unitary matrix V and $\Sigma$ is a m$\times$n diagonal matrix. The entries on the diagonal of $\Sigma$, which are non-negativ real numbers, are called the singular values of M. The columns of U are called left singular vectors and the rows of V$^*$ are called right singular vectors of M. \\
SVD can be used for image compression the following way. The singular values are sorted in descending order and then only the largest singular values and the corresponding right and left singular vectors are used to save the image.

\subsection{Quanitization}
\begin{figure}[tbp]
  \centering
  \includegraphics[width=\columnwidth]{bpQuantizer}
  \caption{Neural Network Structure for compression.}
  \label{fig:bpQuantizer}
\end{figure}

Quantisation as shown in Figure~\ref{fig:bpQuantizer} is the process of mapping a large set of values to a smaller set of values. In Figure~\ref{fig:bpQuantizer} we construct 8 possible binary codes which can be represented with 3 bits. Each code represents an interval of the decimal values. For example, for the values between -1.0 and -0.75 we use the code 000. This way, we use 3 bits for a pixel that originally is represented by 8 bits. This compression method is lossy and the original image can not be reconstructed. 

\section{The Compression Algorithm}
\label{sec:compAlg}

\begin{enumerate}
\item Train neural network with 100 images
\item 2 Iterations of re-training on neural network with actual image
\item Compress image using neural network
\item Compress with quanitization

\end{enumerate}

\section{Results}

\begin{figure}
\centering
\begin{tabular}{cccc}
\subfloat {\includegraphics[width=4cm]{original_22}} & 
\subfloat {\includegraphics[width=4cm]{reconstructed_22}} & 
\end{tabular}
\caption{Visual comparison of original and reconstructed image.}
\label{fig:compare_images}
\end{figure}


We tested the compression algorithm with 100 images which were not in the training set and get a mean error of (......) and a compression rate of (......). 
\newline
Figure~\ref{fig:compare_images} shows an example image before any compression on the left and the reconstructed image after compression on the right. 




\section{Discussion}

We also tried to apply SVD on the image and then compress the U and V matrices with our neural network. The training had to be adapted. But the decompressed U and V matrices looked very different than the original ones. \newline

Strengths: Mean error, Compression rate. \newline
Weakness: Time


\section{Computational Intelligence Laboratory Requirements}
\label{sec:cil}
\subsection{Developing a Novel Solution}

\subsection{Comparison to Baselines}

\subsection{Write Up}



\subsubsection{Installation}

\subsubsection{Compiling \LaTeX{}}

\subsubsection{Equations}

\subsubsection{Tables and Figures}

\subsection{Grading}

\section{Summary}

\section*{Acknowledgements}


\bibliographystyle{IEEEtran}
\bibliography{howto-paper}
\end{document}
