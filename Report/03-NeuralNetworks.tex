\section{Neural Networks}
\label{sec:nn}

Inspired by biological nervous systems, such as the human brain, artificial neural networks are used for information processing. In biological nervous systems, highly interconnected neurons work together to solve a specific task. Each neuron solves a subtask and communicates with other neurons. In artificial neural networks this idea is used to approximate arbitrary  non-linear functions. The nodes (neurons) are connected by weighted edges and the topology is chosen depending on the problem. In the training phase, the weights of the edges are set depending on the data. Neural networks are often represented in layers - each layer contains some of the nodes and then follows the interconnections to the next layer. These connections add high flexibility and the potential to self-organize.

The power of neural networks lies in its ability to learn and generalize \cite{Haykin:1998:NNC:521706}. That means that even for data not encountered during training, the network produces reasonable outputs. 

\subsection{Neural Network Structure}
\label{sec:neural_net_structure}
\begin{figure}[tbp]
  \centering
  \includegraphics[width=\columnwidth]{images/nnStructure}
  \caption{Neural Network Structure for compression.}
  \label{fig:nnStructure}
\end{figure}

Figure~\ref{fig:nnStructure} shows the neural network structure we use for compression. There are three types of layers: input layer, hidden layer and output layer. The input layer and the hidden layer are fully interconnected as are the hidden layer and the output layer. The weights of the edges are determined by the training algorithm which is described in the next section. This kind of neural network is also called bottleneck-like neural network, because the hidden layer is a bottleneck.

{\color{red} TODO: RELATED WORK}

\subsection{Compression with Neural Networks}

Most compression algorithms that use neural networks fall into one of three categories\cite{Dony1995}: 

\subsubsection{Predictive Coding}
Predictive coding aims to reduce redundancies by decorrelating the data. An estimate \(\hat{x}(n)\) is calculated by applying a predictor to the current pixel and neighboring pixels, i.e. by
\begin{equation}
\hat{x}_n = \sum_{j=1}^{p} w_j x(n-j)
\end{equation}
where \({w_j}\) is a set of coefficients from a statistical model called autoregressive model. The mean squared error is then minimized by satisfying \(Rw = d\) where \(R_{ij}\) is the \emph{ij}th element of the autocovariance matrix and \(d_j = E[\hat{x}(n)x(j)]\).
\subsubsection{Transform coding}
asdf

\subsubsection{Vector quantization}
asdf
