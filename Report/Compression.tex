\section{Compression}

\subsection{Compression with neural networks}
We use the topology described in section~\ref{sec:neural_net_structure}. This network is then trained with a set of random images. Then, the image can be compressed with this network in the following way. Chunks of the image are iterated in turn and every chunk is fed into the neural network from the left hand side. If we have 8$\times$8 chunks, we have 64 pixels and each pixel is given to one node on the input layer(which in this case has 64 nodes) of the neural network. Then the values on the hidden layer are taken as the "compressed" values. Since the hidden layer has fewer nodes than the input layer, this gives a compression. As one can see in Figure ~\ref{fig:nnStructure}, the neural net used here has 16 nodes on the hidden layer. This already gives a compression ratio of 0.25. 

\subsection{Compression with quantization}
To further improve compression, quantization as described in Section~\ref{sec:quanitization} is used. The output of the neural network is quantized, such that each 8 bit value is represented using 3 bits.  

\subsection{The Compression Algorithm}
\label{sec:compAlg}

Overall, the compression can be summarised as follows.

\begin{enumerate}
\item The neural network is trained with the training set; in this case 100 images.
\item The trained network is adjusted to the actual image by selecting a few random chunks of the image. Then, the the training is done with those few chunks and the weights updated. 
\item Compress 8$\times$ chunks of image in turn using neural network.
\item Compress all the resulting values with quanitization.

\end{enumerate}
